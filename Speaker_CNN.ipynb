{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield int(start), int(start + window_size)\n",
    "        start += (window_size / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_features(parent_dir,sub_dirs1,sub_dirs2,file_ext=\"*\",bands = 60, frames = 41):\n",
    "    window_size = 512 * (frames - 1)\n",
    "    log_specgrams = []\n",
    "    labels = []\n",
    "    for d_name in sub_dirs1:\n",
    "        #print(d_name,sub_dirs2)\n",
    "        for l, sub_dir in enumerate(sub_dirs2):\n",
    "            #print(sub_dir)\n",
    "            for fn in glob.glob(os.path.join(parent_dir,d_name,sub_dir, file_ext)):\n",
    "                #print(fn)\n",
    "                sound_clip,s = librosa.load(fn)\n",
    "                label = fn.split('/')[3].split('_')[0]\n",
    "                print(fn)\n",
    "                for (start,end) in windows(sound_clip,window_size):\n",
    "                    if(len(sound_clip[start:end]) == window_size):\n",
    "                        signal = sound_clip[start:end]\n",
    "                        melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n",
    "                        logspec = librosa.logamplitude(melspec)\n",
    "                        logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "                        log_specgrams.append(logspec)\n",
    "                        labels.append(label)\n",
    "\n",
    "    log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "    for i in range(len(features)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "    return np.array(features), np.array(labels,dtype = np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset/rachit/f1/37_rachit_18.wav\n",
      "New Dataset/rachit/f1/37_rachit_19.wav\n",
      "New Dataset/rachit/f1/37_rachit_20.wav\n",
      "New Dataset/rachit/f1/37_rachit_21.wav\n",
      "New Dataset/rachit/f1/37_rachit_22.wav\n",
      "New Dataset/rachit/f1/37_rachit_23.wav\n",
      "New Dataset/rachit/f1/37_rachit_24.wav\n",
      "New Dataset/rachit/f1/37_rachit_25.wav\n",
      "New Dataset/rachit/f1/37_rachit_26.wav\n",
      "New Dataset/rachit/f1/37_rachit_27.wav\n",
      "New Dataset/rachit/f1/37_rachit_28.wav\n",
      "New Dataset/rachit/f1/37_rachit_29.wav\n",
      "New Dataset/rachit/f1/37_rachit_30.wav\n",
      "New Dataset/rachit/f1/37_rachit_31.wav\n",
      "New Dataset/rachit/f1/37_rachit_32.wav\n",
      "New Dataset/rachit/f1/37_rachit_33.wav\n",
      "New Dataset/rachit/f1/37_rachit_34.wav\n",
      "New Dataset/rachit/f1/37_rachit_35.wav\n",
      "New Dataset/rachit/f1/37_rachit_36.wav\n",
      "New Dataset/rachit/f1/37_rachit_37.wav\n",
      "New Dataset/rachit/f1/37_rachit_01.wav\n",
      "New Dataset/rachit/f1/37_rachit_02.wav\n",
      "New Dataset/rachit/f1/37_rachit_03.wav\n",
      "New Dataset/rachit/f1/37_rachit_04.wav\n",
      "New Dataset/rachit/f1/37_rachit_05.wav\n",
      "New Dataset/rachit/f1/37_rachit_06.wav\n",
      "New Dataset/rachit/f1/37_rachit_07.wav\n",
      "New Dataset/rachit/f1/37_rachit_08.wav\n",
      "New Dataset/rachit/f1/37_rachit_09.wav\n",
      "New Dataset/rachit/f1/37_rachit_10.wav\n",
      "New Dataset/rachit/f1/37_rachit_11.wav\n",
      "New Dataset/rachit/f1/37_rachit_12.wav\n",
      "New Dataset/rachit/f1/37_rachit_13.wav\n",
      "New Dataset/rachit/f1/37_rachit_14.wav\n",
      "New Dataset/rachit/f1/37_rachit_15.wav\n",
      "New Dataset/rachit/f1/37_rachit_16.wav\n",
      "New Dataset/rachit/f1/37_rachit_17.wav\n",
      "New Dataset/abhi/f1/0_Abhi (26).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (42).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (10).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (11).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (12).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (13).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (14).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (15).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (16).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (17).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (18).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (19).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (2).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (20).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (21).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (22).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (23).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (24).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (25).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (27).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (28).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (29).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (3).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (30).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (31).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (32).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (33).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (34).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (35).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (36).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (37).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (38).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (39).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (4).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (40).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (41).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (43).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (44).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (45).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (46).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (47).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (48).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (49).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (5).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (50).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (51).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (52).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (53).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (54).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (55).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (56).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (57).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (58).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (59).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (6).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (60).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (61).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (62).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (63).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (64).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (7).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (8).WAV\n",
      "New Dataset/abhi/f1/0_Abhi (9).WAV\n",
      "New Dataset/abhi/f1/0_Abhi.WAV\n"
     ]
    }
   ],
   "source": [
    "parent_dir = 'New Dataset'\n",
    "sub_dirs1=[\"rachit\",\"abhi\"]\n",
    "sub_dirs2 = [\"f1\"]\n",
    "features,labels = extract_features(parent_dir,sub_dirs1,sub_dirs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2193, 60, 41, 2)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4b69f1b8ed70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features_updated_CNN.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "f = open('Features_updated_CNN.pickle', 'wb')\n",
    "pickle.dump(features_1, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('Updated_CNN_Features.h5', 'w')\n",
    "h5f.create_dataset('dataset_1', data=features_1)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('Updated_CNN_Lables.h5', 'w')\n",
    "h5f.create_dataset('dataset_2', data=labels_1)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9f1cc4157c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Lables_updated_CNN.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_1' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "f = open('Lables_updated_CNN.pickle', 'wb')\n",
    "pickle.dump(labels_1, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_file = open('Lables_CNN.pickle', 'rb')\n",
    "labels_1=pickle.load(pkl_file)\n",
    "pkl_file = open('Features_CNN.pickle', 'rb')\n",
    "features_1=pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72100, 60, 41, 2)\n",
      "(2193, 60, 41, 2)\n"
     ]
    }
   ],
   "source": [
    "print(features_1.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_1=np.concatenate((features_1,features),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74293, 60, 41, 2)\n"
     ]
    }
   ],
   "source": [
    "print(features_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74293,)\n"
     ]
    }
   ],
   "source": [
    "labels_1=np.concatenate((labels_1,labels),axis=0)\n",
    "print(labels_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i,j in enumerate(labels):\n",
    "    if j==36:\n",
    "        labels[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = one_hot_encode(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(labels[71500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(1.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def apply_convolution(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([kernel_size, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth])\n",
    "    return tf.nn.relu(tf.add(conv2d(x, weights),biases))\n",
    "\n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, kernel_size, kernel_size, 1], \n",
    "                          strides=[1, stride_size, stride_size, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rnd_indices = np.random.rand(len(labels)) < 0.70\n",
    "train_x = features[rnd_indices]\n",
    "train_y = labels[rnd_indices]\n",
    "test_x = features[~rnd_indices]\n",
    "test_y = labels[~rnd_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# frames = 41\n",
    "bands = 60\n",
    "\n",
    "feature_size = 2460 #60x41\n",
    "num_labels = 36\n",
    "num_channels = 2\n",
    "\n",
    "batch_size = 1000\n",
    "kernel_size = 30\n",
    "depth = 20\n",
    "num_hidden = 200\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,bands,frames,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "cov = apply_convolution(X,kernel_size,num_channels,depth)\n",
    "\n",
    "shape = cov.get_shape().as_list()\n",
    "cov_flat = tf.reshape(cov, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights = weight_variable([shape[1] * shape[2] * depth, num_hidden])\n",
    "f_biases = bias_variable([num_hidden])\n",
    "f = tf.nn.sigmoid(tf.add(tf.matmul(cov_flat, f_weights),f_biases))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "session=tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "for itr in range(training_iterations):    \n",
    "    print(itr,end=\" \")\n",
    "    offset = (itr * batch_size) % (train_y.shape[0] - batch_size)\n",
    "    batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "    batch_y = train_y[offset:(offset + batch_size), :]\n",
    "\n",
    "    _, c = session.run([optimizer, cross_entropy],feed_dict={X: batch_x, Y : batch_y})\n",
    "    cost_history = np.append(cost_history,c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for itr in range(training_iterations):    \n",
    "    print(itr,end=\" \")\n",
    "    offset = (itr * batch_size) % (train_y.shape[0] - batch_size)\n",
    "    batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "    batch_y = train_y[offset:(offset + batch_size), :]\n",
    "\n",
    "    _, c = session.run([optimizer, cross_entropy],feed_dict={X: batch_x, Y : batch_y})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_size=test_x.shape[0]\n",
    "print(test_size)\n",
    "print('Test accuracy: ',round(session.run(accuracy, feed_dict={X: test_x[:100], Y: test_y[:100]}) , 3))    # s.run() always returns a tuple\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print('Test accuracy: ',round(session.run(accuracy, feed_dict={X: test_x, Y: test_y}) , 3))\n",
    "\n",
    "n=(int)(test_size/batch_size)\n",
    "for step in range((int)(test_size / batch_size)):\n",
    "    offset = step * batch_size\n",
    "    batch_data = test_x[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = test_y[offset:(offset + batch_size)]\n",
    "    #print(batch_data,batch_labels)\n",
    "    #feed_dict = { X:batch_data,Y:batch_size}\n",
    "    print('Test accuracy: ',round(session.run(accuracy, feed_dict={X: batch_data, Y: batch_labels}) , 3))    # s.run() always returns a tuple\n",
    "    #(predictions,) = s.run([test_prediction],feed_dict=feed_dict)\n",
    "    #correct += numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(batch_labels, 1))\n",
    "    #test_error = 100.0 - (100.0 * (correct / float(test_size)))\n",
    "    # Finally print the result!\n",
    "    #test_error = error_rate(test_prediction.eval(), test_labels)\n",
    "    #print('Test error: %.1f%%' %test_error)\n",
    "     #if FLAGS.self_test:\n",
    "        #print('test_error', test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.plot(cost_history)\n",
    "plt.axis([0,training_iterations,0,np.max(cost_history)])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
